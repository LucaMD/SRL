{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PIPELINE for MetaVision VUmc Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "################################################################################################################\n",
    "### import libraries\n",
    "import os\n",
    "import platform\n",
    "import copy\n",
    "import sys\n",
    "import pyodbc\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools \n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "# import from parent directory with a little help from sys.path.insert()\n",
    "#sys.path.insert(0, '..\\\\src\\\\') # Windows\n",
    "sys.path.insert(0, '../src')  # OSX / Linux\n",
    "\n",
    "### from util.py (file which once contained all classes and functions):\n",
    "from util import * # automatically reload python (e.g. util.py) file when they are changed.\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Configuration file to determine root directory \n",
    "import conf\n",
    "\n",
    "# from configuration file set working directory\n",
    "main_path = os.path.join(conf.ROOT_DIR, 'SEPSIS')\n",
    "os.chdir(main_path)\n",
    "\n",
    "# Define the subfolders paths\n",
    "data_path = '\\ICV_data\\\\'\n",
    "query_path = '\\ICV_sql\\\\'\n",
    "source_path = '\\ICV_src\\\\'\n",
    "admission_path = '\\ICV_data\\\\Admission\\\\'\n",
    "chunk_data_path = '\\ICV_data\\\\Chunks\\\\'\n",
    "\n",
    "############################################################################\n",
    "\"\"\"\n",
    "SQL CONNECTION to MetaVision Databse using PYODBC\n",
    "\"\"\"\n",
    "# Check connection query:\n",
    "sql = \"SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'\"\n",
    "\n",
    "# Define MV_connection\n",
    "PYODBC_con = pyodbc.connect('Trusted_Connection=yes', driver = '{ODBC Driver 13 for SQL Server}',\n",
    "                            server = 'server', database = 'db', Timeout=20)\n",
    "# check connection, print list of tables in database\n",
    "PYODBC_check = pd.read_sql_query(sql, PYODBC_con)\n",
    "\n",
    "# PYODBC Query from file function\n",
    "def qfile(query,encoding=None):\n",
    "    import fileinput\n",
    "    with open(main_path + query_path + query,'r', encoding=encoding) as inserts: # ,'r' to open file in \"read only\" mode\n",
    "        sql = \" \".join(line for line in inserts if not line.isspace())\n",
    "        \n",
    "    # execute query\n",
    "    df = pd.read_sql_query(sql, PYODBC_con)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def qptlist(query,pt_list,encoding=None):\n",
    "    import fileinput\n",
    "    with open(main_path + query_path + query,'r', encoding=encoding) as inserts: # ,'r' to open file in \"read only\" mode\n",
    "        sql = \" \".join(line for line in inserts if not line.isspace())\n",
    "        \n",
    "    # replace pt_list by actual pt_list:\n",
    "    pt_list_holder = 'PT_LIST'\n",
    "    sql = sql.replace(pt_list_holder, pt_list)\n",
    "    #print(sql)\n",
    "    \n",
    "    # execute query\n",
    "    df = pd.read_sql_query(sql, PYODBC_con)\n",
    "    \n",
    "    return df\n",
    "\n",
    "############################################################################\n",
    "# Settings for Pandas to display more then the default amount of collumns\n",
    "pd.set_option(\"display.max_columns\",150)\n",
    "\n",
    "# SQL working\n",
    "print(\"SQL connections established!\\n\") \n",
    "\n",
    "### Check everything\n",
    "print_python_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pt_list\n",
    "\n",
    "### Create Chunks of pt_list for VITALS, LABS, FLUIDS, VASOPRESSOR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_list_df = pd.read_csv(main_path + data_path + \"admissions_df.csv\", sep=',')\n",
    "print(\"Number of total Metavision admissions: %d\" % len(pt_list_df.index))\n",
    "\n",
    "# convert pt_list_dataframe_column to a string for future queries\n",
    "pt_list = \"'\" + (\"', '\".join(map(str, pt_list_df['PatientID'].tolist()))) + \"'\"\n",
    "\n",
    "# Check for DUPLICATES!\n",
    "print(\"Number of duplicated Metavision admissions: %d\" % sum(pt_list_df.duplicated('PatientID', keep=False) == True))\n",
    "\n",
    "n = 500  #chunk row size\n",
    "list_df = [pt_list_df[i:i+n] for i in range(0,pt_list_df.shape[0],n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Demographics, SIRS and SOFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note start time\n",
    "print(\"Started at : \" + str(datetime.now()))\n",
    "\n",
    "### Get Queries from file\n",
    "AGE = qptlist('GET_Age_ptlist.sql',pt_list)\n",
    "print('Count of AGE: ' + str(len(AGE)))\n",
    "#print(AGE.head(),'\\n')\n",
    "\n",
    "# Gender\n",
    "GENDER = qptlist('GET_Gender_ptlist.sql',pt_list)\n",
    "print('Count of GENDER: ' + str(len(GENDER)))\n",
    "#print(GENDER.head(),'\\n')\n",
    "\n",
    "# Admissio nweight\n",
    "WEIGHT = qptlist('GET_AdmissionWeight_ptlist.sql',pt_list,'utf-16')\n",
    "print('Count of WEIGHT: ' + str(len(WEIGHT)))\n",
    "#print(WEIGHT.head(),'\\n')\n",
    "\n",
    "# length\n",
    "LENGTH = qptlist('GET_length_ptlist.sql',pt_list)\n",
    "print('Count of LENGTH: ' + str(len(LENGTH)))\n",
    "#print(WEIGHT.head(),'\\n')\n",
    "\n",
    "# YES/NO Ventilation in first 24 hours\n",
    "VENT = qptlist('GET_First24H_ventilator_ptlist.sql',pt_list)\n",
    "print('Count of VENT: ' + str(len(VENT)))\n",
    "#print(VENT.head(),'\\n')\n",
    "\n",
    "# ALIVE [0] OR DEAD [1] at Discharge \n",
    "DEATH = qptlist('GET_Death_ptlist.sql',pt_list)\n",
    "print('Count of DEATH: ' + str(len(DEATH)))\n",
    "#print(DEATH.head(),'\\n')\n",
    "\n",
    "# SIRS !! SLOW QUERY\n",
    "SIRS = qptlist('GET_SIRS_Admission_ptlist.sql',pt_list,'utf-16')\n",
    "print('Count of SIRS: ' + str(len(SIRS)))\n",
    "#print(SIRS.head(),'\\n')\n",
    "\n",
    "# Note end time\n",
    "print(\"Finished at : \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SOFA score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note start time\n",
    "print(\"Started at : \" + str(datetime.now()))\n",
    "\n",
    "# Define SOFA SQL subfilter directory\n",
    "SOFA_Filter_path = '\\\\util\\\\SOFA_filter\\\\'\n",
    " \n",
    "# SOFA - labs subquery\n",
    "SOFA_labvalues = qptlist(SOFA_Filter_path + 'GET_SOFA_labvalues_ptlist.sql',pt_list)\n",
    "print('Count of Lab values: ' + str(len(SOFA_labvalues)))\n",
    "#print(SOFA_labvalues.head(),'\\n')\n",
    "\n",
    "# SOFA - urine production subquery\n",
    "SOFA_UP = qptlist(SOFA_Filter_path + 'GET_SOFA_UP_ptlist.sql',pt_list)\n",
    "print('Count of Urine Production: ' + str(len(SOFA_UP)))\n",
    "#print(SOFA_UP.head(),'\\n')\n",
    "\n",
    "# SOFA - FiO2 subquery\n",
    "SOFA_FIO2 = qptlist(SOFA_Filter_path + 'GET_SOFA_FIO2_ptlist.sql',pt_list)\n",
    "print('Count of FIO2: ' + str(len(SOFA_FIO2)))\n",
    "#print(SOFA_FIO2.head(),'\\n')\n",
    "\n",
    "# SOFA - EMV subquery\n",
    "SOFA_EMV = qptlist(SOFA_Filter_path + 'GET_SOFA_EMV_ptlist.sql',pt_list)\n",
    "print('Count of EMV: ' + str(len(SOFA_EMV)))\n",
    "#print(SOFA_EMV.head(),'\\n')\n",
    "      \n",
    "# SOFA - vsopressor dose subquery\n",
    "SOFA_vasodose = qptlist(SOFA_Filter_path + 'GET_SOFA_vasodose_ptlist.sql',pt_list)\n",
    "print('Count of Vasoscore: ' + str(len(SOFA_vasodose)))\n",
    "#print(SOFA_vasodose.head(),'\\n')\n",
    "\n",
    "# Merge tables to one file\n",
    "SOFA_data_frames = [SOFA_labvalues,SOFA_UP,SOFA_FIO2,SOFA_vasodose,SOFA_EMV]\n",
    "\n",
    "# using functools.reduce merge all dataframe. (pd.merge() only merges two dataframes at a time)\n",
    "SOFA = reduce(lambda left,right: pd.merge(left,right,on=['PatientID'],\n",
    "                                            how='outer'), SOFA_data_frames)\n",
    "\n",
    "# Note end time\n",
    "print(\"Finished at : \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE DEMOGRAPHICS + SIRS + SOFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables to one file\n",
    "data_frames = [AGE,GENDER,LENGTH,WEIGHT,VENT,SIRS,SOFA,DEATH]\n",
    "\n",
    "# using functools.reduce merge all dataframe. (pd.merge() only merges two dataframes at a time)\n",
    "demographics = reduce(lambda left,right: pd.merge(left,right,on=['PatientID'],\n",
    "                                            how='outer'), data_frames)\n",
    "\n",
    "# Save to demographics.csv\n",
    "demographics.to_csv(os.path.join(main_path + data_path, 'demographics.csv'),sep=',',index=False)\n",
    "\n",
    "print(\"Finished at : \" + str(datetime.now()))\n",
    "\n",
    "# visual inspection\n",
    "demographics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vitals in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TEST DATASET -  FULL QUERY: Get Queries from file\n",
    "# Vitals = qptlist('Get_Vitals_ptlist.sql',pt_list,'utf-16')\n",
    "# ## Save to csv\n",
    "# Vitals.to_csv(os.path.join(main_path + data_path, 'Vitals.csv'),sep=',',index=False)\n",
    "\n",
    "# Note start time\n",
    "print(\"Started at : \" + str(datetime.now()))\n",
    "\n",
    "# Get vitals in chunks\n",
    "i=0\n",
    "for list in list_df:\n",
    "    # count\n",
    "    i=i+1\n",
    "    \n",
    "    # convert pt_list_dataframe_column to a string for future queries\n",
    "    pt_list_subset = \"'\" + (\"', '\".join(map(str, list['PatientID'].tolist()))) + \"'\"\n",
    "    \n",
    "    # execute query for subset of patients\n",
    "    print('Chuck: ' + str(i) + ' out of ' + str(len(list_df)) + \". Started at : \" + str(datetime.now()))\n",
    "    Vitals = qptlist('Get_Vitals_ptlist.sql',pt_list_subset)#,'utf-16')\n",
    "\n",
    "    # Save to csv\n",
    "    Vitals.to_csv(os.path.join(main_path + chunk_data_path, 'Vitals/Vitals_Chunk_' + str(i) + '.csv'),sep=',',index=False)\n",
    "\n",
    "# note end time\n",
    "print(\"Finished at : \" + str(datetime.now()))\n",
    "\n",
    "# visual inspection\n",
    "Vitals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Lab Signals in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST DATASET -  FULL QUERY: Get Queries from file\n",
    "# labs = qptlist('Get_labs_ptlist.sql',pt_list,'utf-16')\n",
    "## Save to csv\n",
    "# labs.to_csv(os.path.join(main_path + data_path, 'Labs.csv'),sep=',',index=False)\n",
    "\n",
    "# Note start time\n",
    "print(\"Started at : \" + str(datetime.now()))\n",
    "\n",
    "# Get labs in chunks\n",
    "i=0\n",
    "for list in list_df:\n",
    "    # count\n",
    "    i=i+1\n",
    "    \n",
    "    # convert pt_list_dataframe_column to a string for future queries\n",
    "    pt_list_subset = \"'\" + (\"', '\".join(map(str, list['PatientID'].tolist()))) + \"'\"\n",
    "    \n",
    "    # execute query for subset of patients\n",
    "    print('Chuck: ' + str(i) + ' out of ' + str(len(list_df)) + \". Started at : \" + str(datetime.now()))\n",
    "    labs = qptlist('Get_labs_ptlist.sql',pt_list_subset,'utf-16')\n",
    "\n",
    "    # Save to csv\n",
    "    labs.to_csv(os.path.join(main_path + chunk_data_path, 'Labs/Labs_Chunk_' + str(i) + '.csv'),sep=',',index=False)\n",
    "        \n",
    "# note end time\n",
    "print(\"Finished at : \" + str(datetime.now()))\n",
    "\n",
    "# visual inspection\n",
    "labs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fluids RangeSignals in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TEST DATASET -  FULL QUERY: Get Queries from file\n",
    "# fluids = qptlist('Get_fluids_ptlist.sql',pt_list)\n",
    "# ## Save to csv\n",
    "# fluids.to_csv(os.path.join(main_path + data_path, 'Fluids.csv'),sep=',',index=False)\n",
    "\n",
    "# Note start time\n",
    "print(\"Started at : \" + str(datetime.now()))\n",
    "\n",
    "# Get fluids in chunks\n",
    "i=0\n",
    "for list in list_df:\n",
    "    # count\n",
    "    i=i+1\n",
    "    \n",
    "    # convert pt_list_dataframe_column to a string for future queries\n",
    "    pt_list_subset = \"'\" + (\"', '\".join(map(str, list['PatientID'].tolist()))) + \"'\"\n",
    "    \n",
    "    # execute query for subset of patients\n",
    "    print('Chuck: ' + str(i) + ' out of ' + str(len(list_df)) + \". Started at : \" + str(datetime.now()))\n",
    "    fluids = qptlist('Get_fluids_ptlist.sql',pt_list_subset)\n",
    "\n",
    "    # Save to csv\n",
    "    fluids.to_csv(os.path.join(main_path + chunk_data_path, 'Fluids/Fluids_Chunk_' + str(i) + '.csv'),sep=',',index=False)\n",
    "\n",
    "# note end time\n",
    "print(\"Finished at : \" + str(datetime.now()))\n",
    "\n",
    "# visual inspection\n",
    "fluids.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Vasopressor RangeSignals in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note start time\n",
    "print(\"Started at : \" + str(datetime.now()))\n",
    "\n",
    "# Get vitals in chunks\n",
    "i=0\n",
    "for list in list_df:\n",
    "    # count\n",
    "    i=i+1\n",
    "    \n",
    "    # convert pt_list_dataframe_column to a string for future queries\n",
    "    pt_list_subset = \"'\" + (\"', '\".join(map(str, list['PatientID'].tolist()))) + \"'\"\n",
    "    \n",
    "    # execute query for subset of patients\n",
    "    print('Chuck: ' + str(i) + ' out of ' + str(len(list_df)) + \". Started at : \" + str(datetime.now()))\n",
    "    vasopressor = qptlist('Get_vasopressor_ptlist.sql',pt_list_subset)\n",
    "    \n",
    "    # Save to \n",
    "    vasopressor.to_csv(os.path.join(main_path + chunk_data_path, 'Vasopressor/Vasopressor_Chunk_' + str(i) + '.csv'),sep=',',index=False)\n",
    "\n",
    "# note end time\n",
    "print(\"Finished at : \" + str(datetime.now()))\n",
    "\n",
    "# visual inspection\n",
    "vasopressor.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get weight for dose conversion from mcg/min to mcg/kg/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admissionweight (for conversion of vasopressor dose in mcg/min to mcg/kg/min)\n",
    "WEIGHT = qptlist('GET_AdmissionWeight_ptlist.sql',pt_list,'utf-16')\n",
    "print('Count of WEIGHT: ' + str(len(WEIGHT)))\n",
    "print(WEIGHT.head(),'\\n')\n",
    "# Save to csv\n",
    "WEIGHT.to_csv(os.path.join(main_path + data_path, 'Weight.csv'),sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get UrineOutput Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admissionweight\n",
    "URINE = qptlist('GET_UrineProduction_ptlist.sql',pt_list)\n",
    "print('Count of URINE: ' + str(len(URINE)))\n",
    "print(URINE.head(),'\\n')\n",
    "# Save to csv\n",
    "URINE.to_csv(os.path.join(main_path + data_path, 'UrineOutput.csv'),sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Notebook\n",
    "    Last step: safely close connection to the database (only applies to PYODBC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
