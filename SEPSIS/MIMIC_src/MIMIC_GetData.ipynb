{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK MIMIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "################################################################################################################\n",
    "### import libraries\n",
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import os\n",
    "import platform\n",
    "import copy\n",
    "import sys\n",
    "import pyodbc\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools \n",
    "import psycopg2\n",
    "import socket\n",
    "import sys\n",
    "import getpass\n",
    "import time\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "# import from parent directory with a little help from sys.path.insert()\n",
    "sys.path.insert(0, '../src') \n",
    "\n",
    "# Settings\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "### Configuration file to determine root directory \n",
    "import conf\n",
    "\n",
    "### check for GPU's\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "### Check everything\n",
    "conf.print_python_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MIMIC data direcotries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from configuration file set working directory\n",
    "main_path = os.path.join(conf.ROOT_DIR, 'SEPSIS')\n",
    "\n",
    "# Define the subfolders paths\n",
    "data_path = 'data'\n",
    "MIMIC_data_path = 'MIMIC_data'\n",
    "query_path = 'MIMIC_sql'\n",
    "source_path = 'MIMIC_src'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MIMIC database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "\"\"\"\n",
    "SQL CONNECTION to MIMIC Database using psycopg2\n",
    "\"\"\"\n",
    "# Connect to local postgres version of mimic\n",
    "sqluser = 'postgres'\n",
    "dbname = 'mimic' \n",
    "schema_name = 'mimiciii'\n",
    "query_schema = 'SET search_path to public,' + schema_name + ';'\n",
    "con = psycopg2.connect(dbname=dbname, user=sqluser, password=\"postgres\")\n",
    "\n",
    "# Query that is usefull when no return table is expected. For example for creating views.\n",
    "def execute_query_safely(sql, con):\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    # try to execute the query\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "    except:\n",
    "        # if an exception, rollback, rethrow the exception - finally closes the connection\n",
    "        cur.execute('rollback;')\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "print('MIMIC - Using username {}'.format(sqluser))\n",
    "print('Connected to postgres {}.{}.{}!'.format(int(con.server_version/10000),\n",
    "                                              (con.server_version - int(con.server_version/10000)*10000)/100,\n",
    "                                              (con.server_version - int(con.server_version/100)*100)))\n",
    "\n",
    "############################################################################\n",
    "# Settings for Pandas to display more then the default amount of collumns\n",
    "pd.set_option(\"display.max_columns\",150)\n",
    "\n",
    "# SQL working\n",
    "print(\"SQL connections established!\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute make-tables to create the SEPSIS COHORT from the MIMIC Github code repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# check if the sepsis3_cohort table exists ... if not we must generate it\n",
    "query = \"\"\"\n",
    "SELECT EXISTS(SELECT 1 FROM information_schema.tables \n",
    "              WHERE table_catalog = '{}'\n",
    "              AND table_schema in ('public','{}')\n",
    "              AND table_name = 'sepsis3');\n",
    "\"\"\".format(dbname, schema_name)\n",
    "tbl_exists = pd.read_sql_query(query, con)\n",
    "tbl_exists = tbl_exists.loc[0,'exists']\n",
    "if tbl_exists:\n",
    "    print('Found the `sepsis3` table. Skipping generation of data in SQL.')\n",
    "else:\n",
    "    print('Running SQL code to generate tables. This may take some time.')\n",
    "    \n",
    "    # read through the \"make-tables.sql\" file in the sql subfolder\n",
    "    query_path = 'query'\n",
    "    \n",
    "    with open(os.path.join(main_path, query_path, 'make-tables.sql'), 'r',encoding='latin-1') as fp:\n",
    "        for line in fp.readlines():\n",
    "            if len(line)<2:\n",
    "                print(line,end='')\n",
    "                continue\n",
    "            \n",
    "            if line[0:2] != '\\i':\n",
    "                print(line,end='')\n",
    "                continue\n",
    "                \n",
    "            # lines which begin with '\\i' call SQL files that generate tables\n",
    "            query_file = os.path.join(main_path, query_path, line[3:].replace('\\n',''))\n",
    "            print('Running {} ...'.format(query_file), end=' ')\n",
    "            with open(query_file, 'r') as fp_query:\n",
    "                query = ''.join(fp_query.readlines())\n",
    "            execute_query_safely(query_schema + query, con)\n",
    "            print('done.')\n",
    "    execute_query_safely(query_schema + 'COMMIT;',con)\n",
    "    print('extra commit executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusion criteria:\n",
    "#   - less than 16 years old\n",
    "#   - never have any chartevents data (i.e. likely administrative error)\n",
    "#   - not cardiac surgery\n",
    "#   - suspected of infection\n",
    "#   - first ICU stay\n",
    "#   - not a CareVue patient (i.e. admitted 2008-2012)\n",
    "# these exclusion criteria are created in the sepsis3_cohort table\n",
    "query = query_schema + \"select * from sepsis3_cohort\"\n",
    "co = pd.read_sql_query(query,con)\n",
    "co.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final SEPSIS dataset from Postgres with exclusions\n",
    "### Add in useful variabeles\n",
    "We have: ICU intime/outtime, suspected infection time, whether the microbiology culture was positive, some demographics, comorbidities, outcomes, and the severity scores. \n",
    "\n",
    "The severity scores are extracted at a [0, 24] hour window centered around ICU admission - except labs have an extended [-6, 24] hour window (i.e. 'sofa' is extracted in this way).\n",
    "### Save the data to file\n",
    "The dataframes will be loaded directly from a file, rather than the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in final dataset - note we apply the exclusion criteria with excluded=0\n",
    "query = query_schema + \"select * from sepsis3 where excluded = 0\"\n",
    "df = pd.read_sql_query(query,con)\n",
    "#print(df.head())\n",
    "\n",
    "# add the composite outcome\n",
    "df['composite_outcome'] = ( (df['hospital_expire_flag']==1) | (df['icu_los']>=3) ).astype(int)\n",
    "labels = OrderedDict([['suspicion_poe', 'BC + ABX (Prescribed)']])\n",
    "\n",
    "# add some other useful variables\n",
    "df['blood culture'] = (~df['blood_culture_time'].isnull())\n",
    "df['suspicion_poe'] = (~df['suspected_infection_time_poe_days'].isnull())\n",
    "df['abx_poe'] = (~df['antibiotic_time_poe'].isnull())\n",
    "df['sepsis-3'] = ((df['suspicion_poe']==1) & (df['sofa']>=2)).astype(int)\n",
    "df['sofa>=2'] = (df['sofa']>=2).astype(int)\n",
    "\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'sepsis3-df.csv'),sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset with no exclusions\n",
    "\n",
    "for completeness sake, we generate an identical copy of the data, except for all `icustay_id` in MIMIC-III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in final dataset - note we add in the individual exclusion criteria\n",
    "query = query_schema + \"\"\"\n",
    "select ie.subject_id\n",
    ", s.*\n",
    ", co.exclusion_secondarystay\n",
    ", co.exclusion_nonadult\n",
    ", co.exclusion_csurg\n",
    ", co.exclusion_carevue\n",
    ", co.exclusion_early_suspicion\n",
    ", co.exclusion_late_suspicion\n",
    ", co.exclusion_bad_data\n",
    "from sepsis3 s\n",
    "-- add in subject_id\n",
    "inner join icustays ie\n",
    "  on s.icustay_id = ie.icustay_id\n",
    "inner join sepsis3_cohort co\n",
    "  on s.icustay_id = co.icustay_id\n",
    "order by s.icustay_id\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query,con)\n",
    "\n",
    "# add the composite outcome\n",
    "df['composite_outcome'] = ( (df['hospital_expire_flag']==1) | (df['icu_los']>=3) ).astype(int)\n",
    "labels = OrderedDict([['suspicion_poe', 'BC + ABX (Prescribed)']])\n",
    "\n",
    "# add some other useful variables\n",
    "df['blood culture'] = (~df['blood_culture_time'].isnull())\n",
    "df['suspicion_poe'] = (~df['suspected_infection_time_poe_days'].isnull())\n",
    "df['abx_poe'] = (~df['antibiotic_time_poe'].isnull())\n",
    "df['sepsis-3'] = ((df['suspicion_poe']==1) & (df['sofa']>=2)).astype(int)\n",
    "df['sofa>=2'] = (df['sofa']>=2).astype(int)\n",
    "\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'sepsis3-df-no-exclusions.csv'),sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RL Vieuws based on SEPSIS3 Cohort\n",
    "Start with a check if the sepsis3 table exists (should have been created as a materialized view here above). If the sepsis3 table exists, create materialized views for the RL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sepsis3_cohort table exists ... if not we did not generate it, start again from the top of this notebook\n",
    "query = \"\"\"\n",
    "SELECT EXISTS(SELECT 1 FROM information_schema.tables \n",
    "              WHERE table_catalog = '{}'\n",
    "              AND table_schema in ('public','{}')\n",
    "              AND table_name = 'sepsis3');\n",
    "\"\"\".format(dbname, schema_name)\n",
    "tbl_exists = pd.read_sql_query(query, con)\n",
    "tbl_exists = tbl_exists.loc[0,'exists']\n",
    "if tbl_exists:\n",
    "    print('Found the `sepsis3` table. Will now create the RL_dataset.\\n')\n",
    "      # read through the \"make-tables.sql\" file in the sql subfolder\n",
    "    query_path = 'MIMIC_sql'\n",
    "    \n",
    "    with open(os.path.join(main_path, query_path, 'RL_views.sql'), 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            if len(line)<2:\n",
    "                print(line,end='')\n",
    "                continue\n",
    "            \n",
    "            if line[0:2] != '\\i':\n",
    "                print(line,end='')\n",
    "                continue\n",
    "                \n",
    "            # lines which begin with '\\i' call SQL files that generate tables\n",
    "            query_file = os.path.join(main_path, query_path, line[3:].replace('\\n',''))\n",
    "            print('Running {} ...'.format(query_file), end=' ')\n",
    "            with open(query_file, 'r') as fp_query:\n",
    "                query = ''.join(fp_query.readlines())\n",
    "            execute_query_safely(query_schema + query, con)\n",
    "            print('done.')\n",
    "        execute_query_safely(query_schema + 'COMMIT;',con)\n",
    "        print('extra commit executed')\n",
    "else:\n",
    "    print('sepsis3 table not found, cannot continue to create the RL cohort.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF RL VIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RL Sepsis cohort\n",
    "    If the sepsis3 table exists, we start querying the database with these newly created materialized views and we save the output to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_cohort.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'cohort.csv'),sep=',',index=True)\n",
    "print('get_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MIMIC vasopressor doses\n",
    "    --  /* SOURCE:  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3590882/\n",
    "    --    Vasopressor Norepinephrine equivalent dose =\n",
    "    --\t  Norepinephrine    1\n",
    "    --\t  Epinephrine       1\n",
    "    --\t  Dopamine          0.01\n",
    "    --\t  Vasopressin       5*\n",
    "    --\t  Phenylephrine     0.45\n",
    "    --  * Approximate conversion of vasopressin dose in units/min to equivalent norepinephrine dose in mcg/kg/min, normalized to 100kg body weight\n",
    "    --   ALSO: https://www.ncbi.nlm.nih.gov/pubmed/22407285\n",
    "    -- */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_vassopressor_cv.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'vassopressors_cv_cohort.csv'),sep=',',index=False)\n",
    "print('vassopressors_cv_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_vassopressor_mv.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'vassopressors_mv_cohort.csv'),sep=',',index=False)\n",
    "print('vassopressors_mv_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get IV Fluid input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_inputevents_cv.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'inputevents_cv_cohort.csv'),sep=',',index=False)\n",
    "print('inputevents_cv_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_inputevents_mv.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'inputevents_mv_cohort.csv'),sep=',',index=False)\n",
    "print('inputevents_mv_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_labs_cohort.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'labs_cohort.csv'),sep=',',index=False)\n",
    "print('labs_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_vitals_cohort.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'vitals_cohort.csv'),sep=',',index=False)\n",
    "print('vitals_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_demographics_cohort.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'demographics_cohort.csv'),sep=',',index=False)\n",
    "print('demographics_cohort done.')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Urine Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_urineoutput_cohort.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'Urineoutput_cohort.csv'),sep=',',index=False)\n",
    "print('UrineOutput_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get FIO2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query based on the new location and sql files\n",
    "query_file = 'get_FiO2_cohort.sql'\n",
    "with open(os.path.join(main_path, query_path, query_file), 'r') as fp_query:\n",
    "    query = ''.join(fp_query.readlines())\n",
    "df = pd.read_sql_query(query_schema + query, con)\n",
    "df.to_csv(os.path.join(main_path, MIMIC_data_path, 'FiO2_cohort.csv'),sep=',',index=False)\n",
    "print('FiO2_cohort done.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of get_MIMIC_data notebook\n",
    "safely close connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()\n",
    "print(\"connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
