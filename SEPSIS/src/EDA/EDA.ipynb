{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################$#################################\n",
    "################################################################################################################\n",
    "### import overall usefull libraries\n",
    "import os\n",
    "import platform\n",
    "import copy\n",
    "import sys\n",
    "import inspect\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "### import specific libraries for this project\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### import visualisation libraries\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "# import from parent directory with a little help from sys.path.insert()\n",
    "sys.path.insert(0, '..') \n",
    "\n",
    "### from util.py (file which once contained all classes and functions):\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from util import * # automatically reload python (e.g. util.py) file when they are changed.\n",
    "\n",
    "### Configuration file to determine root directory \n",
    "import conf\n",
    "\n",
    "### check for GPU's\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "### Check everything\n",
    "conf.print_python_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment name\n",
    "exp_name = 'FINAL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "################################################################################################################\n",
    "# This is horrible practice: https://stackoverflow.com/questions/2052390/manually-raising-throwing-an-exception-in-python\n",
    "if not os.path.exists(os.path.join(conf.EXP_DIR, exp_name)):\n",
    "    raise Exception('Cannot find experiment directory, run create_exp_dataset prior to running this file')\n",
    "else:\n",
    "    exp_dir = os.path.join(conf.EXP_DIR, exp_name)\n",
    "    ############################################\n",
    "    # load data files    \n",
    "    try:\n",
    "        data_dict = joblib.load(os.path.join(exp_dir, 'data/FINAL_data_dict.pkl'))\n",
    "    except:\n",
    "        raise Exception('Cannot load dataset, run the create_exp_dataset Notebook to create new data pickle files ')\n",
    "\n",
    "\n",
    "# inspect it (slightly less shitty code then before, still sorry)\n",
    "print(\"Visual inspection of data dictionary structure:\")\n",
    "for k, v in data_dict.items():\n",
    "    if(k == 'v'): continue\n",
    "    elif(k == 'featurenames'): continue\n",
    "    for k1, v1 in v.items():\n",
    "        print(k, k1)\n",
    "\n",
    "# Get model-ready data\n",
    "mimic_normalised = data_dict['train']['X']\n",
    "icv_normalised = data_dict['test']['X']\n",
    "mimic_n = pd.DataFrame(mimic_normalised, columns=data_dict['featurenames'])\n",
    "icv_n = pd.DataFrame(icv_normalised, columns=data_dict['featurenames'])\n",
    "\n",
    "# Get feature names\n",
    "features = data_dict['featurenames']\n",
    "print(features)\n",
    "\n",
    "# import ORIGINAL csv files\n",
    "ICV_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_ICV.csv'), sep=',')\n",
    "icv = ICV_data[features]\n",
    "MIMIC_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_MIMIC.csv'), sep=',')\n",
    "mimic = MIMIC_data[features]\n",
    "\n",
    "mimic['dataset'] = 'mimic'\n",
    "icv['dataset'] = 'icv'\n",
    "data = pd.concat([mimic, icv])\n",
    "\n",
    "def ignore_outliers(a, p=0.01):\n",
    "    a = a.dropna()\n",
    "    b = np.quantile(a, p)\n",
    "    c = np.quantile(a, 1-p)\n",
    "    assert b > -np.Infinity\n",
    "    assert c < np.Infinity\n",
    "    a = a[a > b]\n",
    "    a = a[a < c]\n",
    "    return a\n",
    "\n",
    "# state space features dimension\n",
    "data_dict['train']['X'].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortality in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_ICV.csv'), sep=',')\n",
    "MIMIC_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_MIMIC.csv'), sep=',')\n",
    "\n",
    "# import ORIGINAL csv files\n",
    "ICV_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_ICV.csv'), sep=',')\n",
    "icv = ICV_data[features]\n",
    "MIMIC_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_MIMIC.csv'), sep=',')\n",
    "mimic = MIMIC_data[features]\n",
    "\n",
    "print(len(MIMIC_data['PatientID'].unique()))\n",
    "print(len(ICV_data['PatientID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICV MORTALITY\n",
    "print(len(ICV_data['PatientID'].loc[ICV_data['Discharge']>0].unique()))\n",
    "print(len(ICV_data['PatientID'].loc[ICV_data['Discharge']==0].unique()))\n",
    "927/4047*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC MORTALITY\n",
    "print(len(MIMIC_data['PatientID'].loc[MIMIC_data['Discharge']>0].unique()))\n",
    "print(len(MIMIC_data['PatientID'].loc[MIMIC_data['Discharge']==0].unique()))\n",
    "931/7320*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model-ready data\n",
    "mimic_normalised = data_dict['train']['X']\n",
    "icv_normalised = data_dict['test']['X']\n",
    "mimic_n = pd.DataFrame(mimic_normalised, columns=data_dict['featurenames'])\n",
    "icv_n = pd.DataFrame(icv_normalised, columns=data_dict['featurenames'])\n",
    "\n",
    "# Get feature names\n",
    "features = data_dict['featurenames']\n",
    "print(features)\n",
    "\n",
    "# import ORIGINAL csv files\n",
    "ICV_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_ICV.csv'), sep=',')\n",
    "icv = ICV_data[features]\n",
    "MIMIC_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_MIMIC.csv'), sep=',')\n",
    "mimic = MIMIC_data[features]\n",
    "\n",
    "mimic['dataset'] = 'mimic'\n",
    "icv['dataset'] = 'icv'\n",
    "data = pd.concat([mimic, icv])\n",
    "\n",
    "def ignore_outliers(a, p=0.01):\n",
    "    a = a.dropna()\n",
    "    b = np.quantile(a, p)\n",
    "    c = np.quantile(a, 1-p)\n",
    "    assert b > -np.Infinity\n",
    "    assert c < np.Infinity\n",
    "    a = a[a > b]\n",
    "    a = a[a < c]\n",
    "    return a\n",
    "\n",
    "features = ['ALAT', 'ASAT', 'Bili', 'APTT', 'Age', 'Albumine',\n",
    "            'Bicarbonaat', 'Calcium', 'Chloride', 'Creat', 'DIA', \n",
    "            'Glucose', 'HB', 'HeartRate', 'INR', 'Kalium', 'LEU',\n",
    "            'Lactate', 'MAP', 'Magnesium', 'Natrium', 'PaCO2', 'PaO2',\n",
    "            'RespRate', 'SYS', 'Weight']\n",
    "\n",
    "binary_fields = ['Gender','Ventilator']\n",
    "\n",
    "norm_fields= ['Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "    'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "    'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "    'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "    'PF_ratio','Albumine', 'Ion_Ca']\n",
    "\n",
    "log_fields = ['max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR',\n",
    "              'Running_total_IV','total_IV_prev','Running_total_UP','total_UP']\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "    # raw distribution\n",
    "    part1 = ignore_outliers(icv[feature], 0.01)\n",
    "    part2 = ignore_outliers(mimic[feature], 0.01)\n",
    "    \n",
    "    # binary features\n",
    "    if feature in binary_fields:\n",
    "        mimic[feature] = mimic[feature] - 0.5 \n",
    "        icv[feature] = icv[feature] - 0.5 \n",
    "\n",
    "    # gaussian normalised features\n",
    "    elif feature in norm_fields:\n",
    "        m_av = mimic[feature].mean()\n",
    "        m_std = mimic[feature].std()\n",
    "        mimic[feature] = (mimic[feature] - m_av) / m_std\n",
    "        i_av = icv[feature].mean()\n",
    "        i_std = icv[feature].std()\n",
    "        icv[feature] = (icv[feature] - i_av) / i_std\n",
    "    \n",
    "    # log normal fields\n",
    "    elif feature in log_fields: \n",
    "        mimic[feature] = np.log(0.1 + mimic[feature])\n",
    "        icv[feature] = np.log(0.1 + icv[feature])\n",
    "        m_av = mimic[feature].mean()\n",
    "        m_std = mimic[feature].std()\n",
    "        mimic[feature] = (mimic[feature] - m_av) / m_std\n",
    "        i_av = icv[feature].mean()\n",
    "        i_std = icv[feature].std()\n",
    "        icv[feature] = (icv[feature] - i_av) / i_std\n",
    "\n",
    "    # normalised\n",
    "    part3 = ignore_outliers(icv[feature], 0.01)\n",
    "    part4 = ignore_outliers(mimic[feature], 0.01)\n",
    "           \n",
    "    # min-max normalisation\n",
    "    mimic_minimum = np.nanmin(mimic[feature])\n",
    "    icv_minimum = np.nanmin(icv[feature])\n",
    "    mimic_maximum = np.nanmax(mimic[feature])\n",
    "    icv_maximum = np.nanmax(icv[feature])\n",
    "    mimic[feature] = (mimic[feature] - mimic_minimum)/(mimic_maximum-mimic_minimum)\n",
    "    icv[feature] = (icv[feature] - icv_minimum)/(icv_maximum-icv_minimum)\n",
    "    \n",
    "    # min-max normalised\n",
    "    part5 = ignore_outliers(icv[feature], 0.01)\n",
    "    part6 = ignore_outliers(mimic[feature], 0.01)\n",
    "\n",
    "    \n",
    "    # Multiplot dimensions\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(part1, label='AmsterdamUMCdb', bins=100, alpha=0.5, density=True)\n",
    "    plt.hist(part2, label='MIMIC', bins=100, alpha=0.5, density=True)\n",
    "    plt.title(str(\"raw {}\".format(feature)))\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(part3, label='AmsterdamUMCdb', bins=100, alpha=0.5, density=True)\n",
    "    plt.hist(part4, label='MIMIC', bins=100, alpha=0.5, density=True)\n",
    "    plt.title(str(\"Normalised {}\".format(feature)))\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(part5, label='AmsterdamUMCdb', bins=100, alpha=0.5, density=True)\n",
    "    plt.hist(part6, label='MIMIC', bins=100, alpha=0.5, density=True)\n",
    "    plt.title(str(\"min max normalised {}\".format(feature)))   \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole dataset exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figures directory\n",
    "FIG_DIR = os.path.join(conf.ROOT_DIR, 'SEPSIS', 'figures')\n",
    "\n",
    "# data preprocessing (WARNING: hacky)\n",
    "mimic_sofa = MIMIC_data.groupby(\"PatientID\").apply(lambda x: x.sort_values(['interval_start_time'])).reset_index(drop=True).groupby(\"PatientID\").nth(7).Sofa_score # I know this is not correct way to do this, but you do get what i'm doing right? Stop complaining.\n",
    "icv_sofa = ICV_data.groupby(\"PatientID\").apply(lambda x: x.sort_values(['interval_start_time'])).reset_index(drop=True).groupby(\"PatientID\").nth(1).Sofa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOFA scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good old MatplotLib\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.suptitle('SOFA scores',fontsize=40)\n",
    "plt.hist(mimic_sofa, color='red', label='MIMIC', alpha=0.5,bins=24, range=(0.5,24.5))\n",
    "plt.hist(icv_sofa, color='blue', label='AmsterdamUMCdb', alpha=0.5,bins=24, range=(0.5,24.5))\n",
    "plt.xlabel(\"SOFA score\")\n",
    "plt.ylabel(\"Patient Count\")\n",
    "plt.xticks(np.arange(0, 25, step=1))\n",
    "plt.legend(fontsize = 'xx-large',frameon=False)\n",
    "plt.title('Degree of dysfunction of six organ systems', fontsize=20, y=1.005) # just that little bit (upward) offset!)\n",
    "plt.savefig(os.path.join(FIG_DIR, 'SOFA_datasets.png'),dpi=400,transparent=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TSNE data\n",
    "try: \n",
    "    x = np.array(pd.read_csv(os.path.join(conf.DATA_DIR, 'TSNE_MIMIC_data.csv'), sep=','))\n",
    "    y = np.array(pd.read_csv(os.path.join(conf.DATA_DIR, 'TSNE_ICV_data.csv'), sep=','))\n",
    "    print(\"Restoring old TSNE datasets\")\n",
    "except:\n",
    "    print(\"Starting new TSNE analysis, takes 8+ hours, be patient\")\n",
    "    from sklearn.manifold import TSNE\n",
    "    np.random.shuffle(mimic_normalised)\n",
    "    np.random.shuffle(icv_normalised)\n",
    "    both = np.concatenate([mimic_normalised[:130000], icv_normalised[:59000]])\n",
    "\n",
    "    # Oh boy here we go: Dimensionality reduction into 2 (X and Y-axis) components\n",
    "    tsne = TSNE()\n",
    "\n",
    "    # This is going to take 8+ hours, have patience\n",
    "    c = tsne.fit_transform(both, )\n",
    "\n",
    "    # How about we save this TSNE dataset it so this doesn't need to be run again... (Counter: 12)\n",
    "    mimic_c, icv_c = c[:130000], c[130000:]\n",
    "    pd.DataFrame(mimic_c).to_csv(os.path.join(conf.DATA_DIR, 'TSNE_MIMIC_data.csv'), index=False)\n",
    "    pd.DataFrame(icv_c).to_csv(os.path.join(conf.DATA_DIR, 'TSNE_ICV_data.csv'), index=False)\n",
    "\n",
    "    x = mimic_c\n",
    "    y = icv_c\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE PLOT\n",
    "red_patch = mpatches.Patch(color='red', label='MIMIC',alpha=0.5)\n",
    "blue_patch = mpatches.Patch(color='blue', label='AmsterdamUMCdb',alpha=0.5)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x[:,0], x[:,1], c='red', alpha=0.05, label='MIMIC')\n",
    "plt.scatter(y[:,0], y[:,1], c='blue', alpha=0.05, label='AmsterdamUMCdb')\n",
    "plt.suptitle('State space distribution',fontsize=40,y=0.99) # just that little y axis offset (details matter!)\n",
    "plt.title('t-Distributed Stochastic Neighbor Embedded State space', fontsize=15)\n",
    "plt.legend(handles=[red_patch, blue_patch],frameon=False)\n",
    "plt.savefig(os.path.join(FIG_DIR, 'TSNE_datasets.png'),dpi=400,transparent=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
