{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "### import libraries\n",
    "import os\n",
    "import platform\n",
    "import copy\n",
    "import sys\n",
    "import pyodbc\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import sklearn as sk\n",
    "import joblib\n",
    "from fancyimpute import KNN    \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "# automatically reload python fiels (util.py and conf.py) when they are changed.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import from parent directory with a little help from sys.path.insert()\n",
    "sys.path.insert(0, '..') \n",
    "\n",
    "### from util.py (file which once contained all classes and functions):\n",
    "from util import * \n",
    "\n",
    "### Configuration file to determine root directory \n",
    "import conf\n",
    "\n",
    "# from configuration file set working directory\n",
    "os.chdir(os.path.join(conf.ROOT_DIR, 'SEPSIS'))\n",
    "\n",
    "# Define the subfolders paths\n",
    "data_path = '\\data\\\\'\n",
    "\n",
    "############################################################################\n",
    "# Settings for Pandas to display more then the default amount of collumns\n",
    "pd.set_option(\"display.max_columns\",150)\n",
    "\n",
    "### Check everything\n",
    "conf.print_python_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load final ICV and MIMIC datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICV_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_ICV.csv'), sep=',')\n",
    "MIMIC_data = pd.read_csv(os.path.join(conf.DATA_DIR, 'final_MIMIC.csv'), sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "################################################################################################################\n",
    "### Experiment name\n",
    "exp_name = 'FINAL'\n",
    "exp_comment = 'Main Experiment - training a model on half of the ICV data and testing on the full MIMIC dataset'\n",
    "\n",
    "### First create experiment directory if not yet exists\n",
    "if not os.path.exists(os.path.join(conf.EXP_DIR, exp_name)):\n",
    "    os.makedirs(os.path.join(conf.EXP_DIR, exp_name))\n",
    "exp_dir = os.path.join(conf.EXP_DIR, exp_name)\n",
    "\n",
    "## Add data\n",
    "if not os.path.exists(os.path.join(exp_dir, 'data')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'data'))\n",
    "    \n",
    "\n",
    "# Themn add subdirectories\n",
    "if not os.path.exists(os.path.join(exp_dir, 'figures')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'figures'))\n",
    "    \n",
    "if not os.path.exists(os.path.join(exp_dir, 'models')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'models'))\n",
    "\n",
    "if not os.path.exists(os.path.join(exp_dir, 'performance')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'performance'))\n",
    "    \n",
    "if not os.path.exists(os.path.join(exp_dir, 'results')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'results'))\n",
    "\n",
    "if not os.path.exists(os.path.join(exp_dir, 'KNN')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'KNN'))\n",
    "    \n",
    "if not os.path.exists(os.path.join(exp_dir, 'FQI')):\n",
    "    os.makedirs(os.path.join(exp_dir, 'FQI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET EXPERIMENT DATA CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "################################################################################################################\n",
    "config = {'random_SEED': 42,    # God does not play dice\n",
    "          'train_sample': 0.7,  # percentage (0 to 1) of patients reserved for training (remainder used for valiation)\n",
    "          'comment': str(exp_comment)\n",
    "         }\n",
    "config_df = pd.DataFrame(config, index=[0])\n",
    "config_df.to_csv(os.path.join(exp_dir, 'data/' + exp_name + '_dataconfig.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data, pick a cell to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# God does not play dice\n",
    "import random\n",
    "random.seed(config['random_SEED'])\n",
    "\n",
    "####### MIMIC SPLIT\n",
    "# now split into train/validation/test sets\n",
    "unique_ids = MIMIC_data['PatientID'].unique()\n",
    "\n",
    "random.shuffle(unique_ids)\n",
    "train_sample = config['train_sample']\n",
    "train_num = int(len(unique_ids) * train_sample)\n",
    "train_ids = unique_ids[:train_num]\n",
    "val_ids = unique_ids[train_num:]\n",
    "\n",
    "# Create datasets\n",
    "train_set = MIMIC_data.loc[MIMIC_data['PatientID'].isin(train_ids)]\n",
    "val_set = MIMIC_data.loc[MIMIC_data['PatientID'].isin(val_ids)]\n",
    "test_set = ICV_data\n",
    "\n",
    "####### ICV SPLIT\n",
    "# unique_ids = ICV_data['PatientID'].unique()\n",
    "\n",
    "# # Create datasets\n",
    "# train_set = ICV_data.loc[ICV_data['PatientID'].isin(train_ids)]\n",
    "# val_set = ICV_data.loc[ICV_data['PatientID'].isin(val_ids)]\n",
    "# test_set = MIMIC_data\n",
    "\n",
    "# keep a raw data copy\n",
    "train_rawdata = train_set\n",
    "val_rawdata = val_set\n",
    "test_rawdata = test_set\n",
    "\n",
    "print(train_rawdata.shape)\n",
    "print(val_rawdata.shape)\n",
    "print(test_rawdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cap values in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "caps = pd.read_csv(os.path.join(conf.DATA_DIR, 'capping_values.csv'), sep=',',decimal='.')\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for i in caps.index:\n",
    "        param = caps.loc[i,'Parameter']\n",
    "        maxval = caps.loc[i,'maxval']\n",
    "        minval = caps.loc[i,'minval']\n",
    "        print(param,minval,maxval)\n",
    "        train_set[param][train_set[param] >= maxval] = maxval\n",
    "        train_set[param][train_set[param] <= minval] = minval\n",
    "        val_set[param][val_set[param] >= maxval] = maxval\n",
    "        val_set[param][val_set[param] <= minval] = minval\n",
    "        test_set[param][test_set[param] >= maxval] = maxval\n",
    "        test_set[param][test_set[param] <= minval] = minval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data\n",
    "\n",
    "### as Raghu et al 2017:\n",
    "    binary_fields = ['gender','mechvent','re_admission']\n",
    "    \n",
    "    norm_fields= ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index',\n",
    "        'PaO2_FiO2','cumulated_balance_tev', 'elixhauser', 'Albumin', u'CO2_mEqL', 'Ionised_Ca']\n",
    "        \n",
    "    log_fields = ['max_dose_vaso','SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR',\n",
    "                  'input_total_tev','input_4hourly_tev','output_total','output_4hourly', 'bloc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_fields = ['Gender','Ventilator']\n",
    "\n",
    "norm_fields= ['Age','Weight','HeartRate','SYS','MAP','DIA','RespRate','Temp','FiO2',\n",
    "    'Kalium','Natrium','Chloride','Glucose','Magnesium','Calcium','ANION_GAP',\n",
    "    'HB','LEU','Trombo','APTT','Art_PH','PaO2','PaCO2','Height',\n",
    "    'Art_BE','Bicarbonaat','Lactate','Sofa_score','Sirs_score','Shock_Index',\n",
    "    'PF_ratio','Albumine', 'Ion_Ca']\n",
    "\n",
    "log_fields = ['max_VP_prev','SpO2','Ureum','Creat','ALAT','ASAT','Bili','INR',\n",
    "              'Running_total_IV','total_IV_prev','Running_total_UP','total_UP']\n",
    "\n",
    "not_used = ['PatientID', 'interval_start_time', 'interval_end_time', 'Reward', 'Discharge', 'discrete_action','discrete_action_original','total_IV','max_VP']\n",
    "\n",
    "# check if all collumns used for ICV_data\n",
    "print(\"All collumns accounted for excluding: \" + str(not_used) + \" == \" +str(len(ICV_data.columns) - len(not_used)==len(binary_fields)+len(norm_fields)+len(log_fields)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise binary fields\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    train_set[binary_fields] = train_set[binary_fields] - 0.5 \n",
    "    val_set[binary_fields] = val_set[binary_fields] - 0.5 \n",
    "    test_set[binary_fields] = test_set[binary_fields] - 0.5\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal distn fields\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for item in norm_fields:\n",
    "        av = train_set[item].mean()\n",
    "        std = train_set[item].std()\n",
    "        train_set[item] = (train_set[item] - av) / std\n",
    "        val_set[item] = (val_set[item] - av) / std\n",
    "        test_set[item] = (test_set[item] - av) / std\n",
    "        print(item,av.round(3),std.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    # log normal fields\n",
    "    train_set[log_fields] = np.log(0.1 + train_set[log_fields])\n",
    "    val_set[log_fields] = np.log(0.1 + val_set[log_fields])\n",
    "    test_set[log_fields] = np.log(0.1 + test_set[log_fields])\n",
    "    \n",
    "    for item in log_fields:\n",
    "        av = train_set[item].mean()\n",
    "        std = train_set[item].std()\n",
    "        train_set[item] = (train_set[item] - av) / std\n",
    "        val_set[item] = (val_set[item] - av) / std\n",
    "        test_set[item] = (test_set[item] - av) / std\n",
    "        print(item,av.round(3),std.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale all features\n",
    "scalable_fields = copy.deepcopy(binary_fields)\n",
    "scalable_fields.extend(norm_fields)\n",
    "scalable_fields.extend(log_fields)\n",
    "\n",
    "# min-max normalization\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    for col in scalable_fields:\n",
    "        minimum = np.nanmin(train_set[col])\n",
    "        maximum = np.nanmax(train_set[col])\n",
    "        print(col,minimum,maximum)\n",
    "        train_set[col] = (train_set[col] - minimum)/(maximum-minimum)\n",
    "        val_set[col] = (val_set[col] - minimum)/(maximum-minimum)\n",
    "        test_set[col] = (test_set[col] - minimum)/(maximum-minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unique_ids = train_set['PatientID'].unique()\n",
    "train_before_impute = train_set.head(30)\n",
    "for unique_id in train_unique_ids:\n",
    "    X_incomplete = train_set.loc[train_set['PatientID']==unique_id][binary_fields+norm_fields+log_fields]\n",
    "    pd.reset_option('mode.chained_assignment')\n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        train_set.loc[train_set['PatientID']==unique_id,binary_fields+norm_fields+log_fields] = KNN(k=3,verbose=False).fit_transform(X_incomplete) # XX_filled_knn\n",
    "print(\"done\")\n",
    "train_set_after_impute = train_set.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_unique_ids = val_set['PatientID'].unique()\n",
    "val_before_impute = val_set.head(30)\n",
    "for unique_id in val_unique_ids:\n",
    "    X_incomplete = val_set.loc[val_set['PatientID']==unique_id][binary_fields+norm_fields+log_fields]\n",
    "    pd.reset_option('mode.chained_assignment')\n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        val_set.loc[val_set['PatientID']==unique_id,binary_fields+norm_fields+log_fields] = KNN(k=3,verbose=False).fit_transform(X_incomplete) # XX_filled_knn\n",
    "print(\"done\")\n",
    "val_after_impute = val_set.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unique_ids = test_set['PatientID'].unique()\n",
    "test_before_impute = test_set.head(30)\n",
    "for unique_id in test_unique_ids:\n",
    "    X_incomplete = test_set.loc[test_set['PatientID']==unique_id][binary_fields+norm_fields+log_fields]\n",
    "    pd.reset_option('mode.chained_assignment')\n",
    "    with pd.option_context('mode.chained_assignment', None):\n",
    "        test_set.loc[test_set['PatientID']==unique_id,binary_fields+norm_fields+log_fields] = KNN(k=3,verbose=False).fit_transform(X_incomplete) # XX_filled_knn\n",
    "print(\"done\")\n",
    "test_after_impute = test_set.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Row ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.index = pd.RangeIndex(len(train_set.index))\n",
    "val_set.index = pd.RangeIndex(len(val_set.index))\n",
    "test_set.index = pd.RangeIndex(len(test_set.index))\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    # row_id.values\n",
    "    train_set['row_id'] = train_set.index\n",
    "    train_set['row_id_next'] = np.where(train_set['PatientID'].shift(+1) != train_set['PatientID'], train_set['row_id'], train_set['row_id']+1)\n",
    "    train_set['row_id_next']  = train_set['row_id_next'] .astype(int)\n",
    "    train_set['row_id_next'][0] = 1                                    # fix the one shortcoming of the above np.where with .shift() solution\n",
    "    train_set['row_id_next'].iloc[-1] = train_set['row_id'].iloc[-1]   # without changes the above text: fix the second shortcoming of the above method\n",
    "\n",
    "    # row_id.values\n",
    "    val_set['row_id'] = val_set.index\n",
    "    val_set['row_id_next'] = np.where(val_set['PatientID'].shift(+1) != val_set['PatientID'], val_set['row_id'], val_set['row_id']+1)\n",
    "    val_set['row_id_next']  = val_set['row_id_next'] .astype(int)\n",
    "    val_set['row_id_next'][0] = 1                                      # fix the one shortcoming of the above np.where with .shift() solution\n",
    "    val_set['row_id_next'].iloc[-1] = val_set['row_id'].iloc[-1]       # without changes the above text: fix the second shortcoming of the above method\n",
    "    # row_id.values\n",
    "    test_set['row_id'] = test_set.index\n",
    "    test_set['row_id_next'] = np.where(test_set['PatientID'].shift(+1) != test_set['PatientID'], test_set['row_id'], test_set['row_id']+1)\n",
    "    test_set['row_id_next']  = test_set['row_id_next'] .astype(int)\n",
    "    test_set['row_id_next'][0] = 1                                     # fix the one shortcoming of the above np.where with .shift() solution\n",
    "    test_set['row_id_next'].iloc[-1] = test_set['row_id'].iloc[-1]     # without changes the above text: fix the second shortcoming of the above method\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = binary_fields+norm_fields+log_fields\n",
    "feature_df_train = train_set[feature_names]\n",
    "feature_df_val = val_set[feature_names]\n",
    "feature_df_test = test_set[feature_names]\n",
    "\n",
    "v = DictVectorizer(sparse = False)\n",
    "feature_dict_train = feature_df_train.to_dict('records')\n",
    "feature_dict_val = feature_df_val.to_dict('records')\n",
    "feature_dict_test = feature_df_test.to_dict('records')\n",
    "\n",
    "print(np.sort(np.array(feature_df_train.columns)))\n",
    "\n",
    "X_train = v.fit_transform(feature_dict_train)\n",
    "X_val = v.transform(feature_dict_val)\n",
    "X_test = v.transform(feature_dict_test)\n",
    "\n",
    "reward_train = train_set.Reward.values\n",
    "reward_val = val_set.Reward.values\n",
    "reward_test = test_set.Reward.values\n",
    "\n",
    "action_train = train_set.discrete_action.values\n",
    "action_val = val_set.discrete_action.values\n",
    "action_test = test_set.discrete_action.values\n",
    "\n",
    "state_row_id_train       = [int(x) for x in train_set.row_id.values]\n",
    "next_state_row_id_train  = [int(x) for x in  train_set.row_id_next.values]\n",
    "\n",
    "state_row_id_val         = [int(x) for x in val_set.row_id.values]\n",
    "next_state_row_id_val    = [int(x) for x in val_set.row_id_next.values]\n",
    "\n",
    "state_row_id_test        = [int(x) for x in test_set.row_id.values]\n",
    "next_state_row_id_test   = [int(x) for x in test_set.row_id_next.values]\n",
    "\n",
    "output_dict = {'train' : {\n",
    "                    'X' : X_train,\n",
    "                    'action' : action_train,\n",
    "                    'reward' : reward_train,\n",
    "                    'state_id' : state_row_id_train,\n",
    "                    'next_state_id' : next_state_row_id_train\n",
    "                },\n",
    "                'val' : {\n",
    "                    'X' : X_val,\n",
    "                    'action' : action_val,\n",
    "                    'reward' : reward_val,\n",
    "                    'state_id' : state_row_id_val,\n",
    "                    'next_state_id' : next_state_row_id_val\n",
    "                },\n",
    "              'test' : {\n",
    "                    'X' : X_test,\n",
    "                    'action' : action_test,\n",
    "                    'reward' : reward_test,\n",
    "                    'state_id' : state_row_id_test,\n",
    "                    'next_state_id' : next_state_row_id_test\n",
    "                },\n",
    "               'v' : v,\n",
    "               'featurenames': np.sort(np.array(list(feature_dict_train[1].keys())))\n",
    "         }\n",
    "\n",
    "print(len(feature_dict_train))\n",
    "print(len(output_dict['train']['next_state_id']))\n",
    "print(len(output_dict['val']['next_state_id']))\n",
    "print(len(output_dict['test']['next_state_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Pickle and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data to csv files\n",
    "train_rawdata.to_csv(os.path.join(exp_dir, 'data/train_rawdata.csv'), index=False)\n",
    "val_rawdata.to_csv(os.path.join(exp_dir, 'data/val_rawdata.csv'), index=False)\n",
    "test_rawdata.to_csv(os.path.join(exp_dir, 'data/test_rawdata.csv'), index=False)\n",
    "\n",
    "# Save processed data to csv files\n",
    "train_set.to_csv(os.path.join(exp_dir, 'data/train_data.csv'), index=False)\n",
    "val_set.to_csv(os.path.join(exp_dir, 'data/val_data.csv'), index=False)\n",
    "test_set.to_csv(os.path.join(exp_dir, 'data/test_data.csv'), index=False)\n",
    "\n",
    "# Save Pickle for modelling\n",
    "joblib.dump(output_dict, os.path.join(exp_dir, 'data/FINAL_data_dict.pkl'))\n",
    "\n",
    "print(\"\\nFinished at: \" + str(datetime.now()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK PICKLE DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = joblib.load(os.path.join(exp_dir, 'data/FINAL_data_dict_old.pkl'))\n",
    "old = len(data_dict['train']['state_id'])\n",
    "#printf(\"\", len(data_dict['train']['state_id'])\n",
    "data_dict = joblib.load(os.path.join(exp_dir, 'data/FINAL_data_dict.pkl'))\n",
    "new = len(data_dict['train']['state_id'])\n",
    "print(\"old = {0}\\nnew = {1}\".format(old,new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
